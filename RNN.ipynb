{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0DIma33mhHUF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DIma33mhHUF",
    "outputId": "543a3c1b-f3a7-4e61-c1e1-17e20fbd6407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib_inline in c:\\users\\owner\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: traitlets in c:\\users\\owner\\anaconda3\\lib\\site-packages (from matplotlib_inline) (5.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "652d8c27",
   "metadata": {
    "id": "652d8c27"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import joblib as jb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "from torchmetrics import R2Score\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f83fd8f",
   "metadata": {
    "id": "2f83fd8f"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n",
    "\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fd46d7d",
   "metadata": {
    "id": "8fd46d7d"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set the initial hidden and cell states to be all zeros\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "    \n",
    "        # Pass the input through the LSTM layer\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "    \n",
    "        # Select the last output of the LSTM layer as the final output\n",
    "        out = out[:, -1, :]\n",
    "    \n",
    "        # Pass the final output through a fully-connected layer to get the final prediction\n",
    "        out = self.linear(out)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ced330c0",
   "metadata": {
    "id": "ced330c0"
   },
   "outputs": [],
   "source": [
    "# Calculating the loss\n",
    "# loss = nn.CrossEntropyLoss(reduction='none')\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "def evaluate_loss(data_iter, net, devices):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for features, labels in data_iter:\n",
    "        features, labels = features.to(devices[0]), labels.to(devices[0])\n",
    "        outputs = net(features)\n",
    "        l = loss(outputs, labels)\n",
    "        l_sum += l.sum()\n",
    "        n += labels.numel()\n",
    "    return l_sum / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffa1c6f4",
   "metadata": {
    "id": "ffa1c6f4"
   },
   "outputs": [],
   "source": [
    "def loss_plotter(train_losses,test_losses):\n",
    "    plt.figure(figsize = (10,6) )\n",
    "    plt.plot(train_losses, 'r*-', label = \"Train Loss\")\n",
    "    plt.plot(test_losses, 'b*-', label = \"Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"The Loss Value\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "126a1ee4",
   "metadata": {
    "id": "126a1ee4"
   },
   "outputs": [],
   "source": [
    "spo2_vals = jb.load('strided_dataset_spo2_vals.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96aa79b9",
   "metadata": {
    "id": "96aa79b9"
   },
   "outputs": [],
   "source": [
    "stride_dataset = jb.load('strided_dataset.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e542a2b",
   "metadata": {
    "id": "2e542a2b"
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame.from_dict(stride_dataset).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd00d6a7",
   "metadata": {
    "id": "bd00d6a7"
   },
   "outputs": [],
   "source": [
    "dataset_spo2 = pd.DataFrame.from_dict(spo2_vals).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc28492f",
   "metadata": {
    "id": "fc28492f"
   },
   "outputs": [],
   "source": [
    "dataset_spo2 = dataset_spo2/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a7c01ab",
   "metadata": {
    "id": "5a7c01ab"
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = np.split(dataset.sample(\n",
    "    frac=1), [int(.8*len(dataset)), int(.9*len(dataset))])\n",
    "y_train, y_valid, y_test = dataset_spo2.loc[X_train.index], dataset_spo2.loc[\n",
    "    X_valid.index], dataset_spo2.loc[X_test.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64500026",
   "metadata": {
    "id": "64500026"
   },
   "outputs": [],
   "source": [
    "# unroll the data\n",
    "X_train_unrolled = []\n",
    "y_train_unrolled = []\n",
    "for r in X_train.index:\n",
    "    for c in X_train.columns:\n",
    "        # skip values containing NaN\n",
    "        if not np.isnan(X_train.loc[r][c]).any() and not np.isnan(y_train.loc[r][c]):\n",
    "            X_train_unrolled.append(X_train.loc[r][c])\n",
    "            y_train_unrolled.append(y_train.loc[r][c])\n",
    "\n",
    "X_valid_unrolled = []\n",
    "y_valid_unrolled = []\n",
    "for r in X_valid.index:\n",
    "    for c in X_valid.columns:\n",
    "        # skip values containing NaN\n",
    "        if not np.isnan(X_valid.loc[r][c]).any() and not np.isnan(y_valid.loc[r][c]):\n",
    "            X_valid_unrolled.append(X_valid.loc[r][c])\n",
    "            y_valid_unrolled.append(y_valid.loc[r][c])\n",
    "\n",
    "X_test_unrolled = []\n",
    "y_test_unrolled = []\n",
    "for r in X_test.index:\n",
    "    for c in X_test.columns:\n",
    "        # skip values containing NaN\n",
    "        if not np.isnan(X_test.loc[r][c]).any() and not np.isnan(y_test.loc[r][c]):\n",
    "            X_test_unrolled.append(X_test.loc[r][c])\n",
    "            y_test_unrolled.append(y_test.loc[r][c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fd69295",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fd69295",
    "outputId": "37a42096-d47c-4efd-b801-828edd7a121d"
   },
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "X_train, y_train = torch.tensor(X_train_unrolled, dtype=torch.float32), torch.tensor(\n",
    "    y_train_unrolled, dtype=torch.float32)\n",
    "X_valid, y_valid = torch.tensor(X_valid_unrolled, dtype=torch.float32), torch.tensor(\n",
    "    y_valid_unrolled, dtype=torch.float32)\n",
    "X_test, y_test = torch.tensor(X_test_unrolled, dtype=torch.float32), torch.tensor(\n",
    "    y_test_unrolled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c47344f8",
   "metadata": {
    "id": "c47344f8"
   },
   "outputs": [],
   "source": [
    "# reshape y tensors\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_valid = y_valid.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c1d52dc",
   "metadata": {
    "id": "9c1d52dc"
   },
   "outputs": [],
   "source": [
    "# normalize x data\n",
    "X_train_mean = X_train.mean() \n",
    "X_train_std = X_train.std()\n",
    "X_train = (X_train - X_train_mean) / (X_train_std + 1e-7)\n",
    "\n",
    "X_valid_mean = X_valid.mean()\n",
    "X_valid_std = X_valid.std()\n",
    "X_valid = (X_valid - X_valid_mean) / (X_valid_std + 1e-7)\n",
    "\n",
    "X_test_mean = X_test.mean()\n",
    "X_test_std = X_test.std()\n",
    "X_test = (X_test - X_test_mean) / (X_test_std + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6012ded9",
   "metadata": {
    "id": "6012ded9"
   },
   "outputs": [],
   "source": [
    "# convert to torch iterators\n",
    "batch_size = 64\n",
    "train_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(\n",
    "    X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "valid_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(\n",
    "    X_valid, y_valid), batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(\n",
    "    X_test, y_test), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf9572c9",
   "metadata": {
    "id": "bf9572c9"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr, wd = 0.001, 1e-4\n",
    "lr_period, lr_decay = 2, 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7336cd2",
   "metadata": {
    "id": "d7336cd2"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e287376c",
   "metadata": {
    "id": "e287376c"
   },
   "outputs": [],
   "source": [
    "model = RNNModel(input_size=240, hidden_size=32, num_layers=2, output_size=5).to(device)\n",
    "#Setting the loss function\n",
    "cost = nn.L1Loss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "efa3e230",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efa3e230",
    "outputId": "595b3fc2-7ce4-405e-ef31-853f90cdad83"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 1, 32), got [2, 2, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m c_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Pass the input through the LSTM layer\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Select the last output of the LSTM layer as the final output\u001b[39;00m\n\u001b[0;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:693\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    688\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    691\u001b[0m                        ):\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExpected hidden[0] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    696\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:226\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    224\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 1, 32), got [2, 2, 32]"
     ]
    }
   ],
   "source": [
    "summary(model,(1,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9df2bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "f9df2bfd",
    "outputId": "e81d600b-cb54-4b23-92fb-17d15990a032"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 1, 32), got [2, 64, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m cost(outputs, labels)\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mRNNModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m c_0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Pass the input through the LSTM layer\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m out, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Select the last output of the LSTM layer as the final output\u001b[39;00m\n\u001b[0;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:693\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    688\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    691\u001b[0m                        ):\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expected_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExpected hidden[0] size \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, got \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    696\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:226\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    224\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 1, 32), got [2, 64, 32]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    for i, (features, labels) in enumerate(train_iter):\n",
    "        batch_size = features.shape[0]\n",
    "        features = torch.reshape(features, (batch_size,1,240))\n",
    "        labels = torch.reshape(labels, (batch_size,1,1))\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(features)\n",
    "        loss = cost(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        \n",
    "        \n",
    "    for i, (features_test, labels_test) in enumerate(test_iter):\n",
    "        batch_size = features_test.shape[0]\n",
    "        features_test = torch.reshape(features_test, (batch_size,1,240))\n",
    "        labels_test = torch.reshape(labels_test, (batch_size,1,1))\n",
    "        \n",
    "        features_test = features_test.to(device)\n",
    "        labels_test = labels_test.to(device)\n",
    "        outputs_test = model(features_test)\n",
    "        loss_test = cost(outputs_test,labels_test)\n",
    "     \n",
    "    train_losses.append(loss.item()*100)\n",
    "    test_losses.append(loss_test.item()*100)\n",
    "    print(\"1) The Train Loss value with the epoch: \" + str(epoch) + \" is equal to \" + str(loss.item()*100))\n",
    "    print(\"2) The Test Loss value with the epoch: \" + str(epoch) + \" is equal to \" + str(loss_test.item()*100))\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    \n",
    "    if epoch == num_epochs-2:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f892c",
   "metadata": {
    "id": "5f2f892c"
   },
   "outputs": [],
   "source": [
    "loss_plotter(train_losses,test_losses)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
